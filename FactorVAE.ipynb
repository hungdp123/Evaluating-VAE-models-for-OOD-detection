{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from typing import Callable, Tuple, List, Dict\nimport numpy\nimport scipy.stats\nimport cv2\nimport PIL\nimport argparse\nimport math\nimport sys\nimport numpy\nimport torch\nimport torchvision\nfrom torch import nn\nimport os\nfrom scipy.stats import gaussian_kde\nfrom bayes_opt import BayesianOptimization\nfrom sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, roc_curve\n\n\nclass FactorVae(torch.nn.Module):\n    \"\"\"FactorVAE OOD detector.  This class includes both the encoder and\n    decoder portions of the model.\n    \n    Args:\n        n_latent - number of latent dimensions\n        beta - hyperparameter beta to use during training\n        n_chan - number of channels in the input image\n        input_d - height x width tuple of input image size in pixels\n        activation - activation function to use for all hidden layers\n        head2logvar - 2nd distribution parameter learned by encoder:\n            'logvar', 'logvar+1', 'var', or 'neglogvar'.\n        interpolation - PIL image interpolation method on resize\n    \"\"\"\n\n    def __init__(self,\n                 n_latent: int,\n                 n_chan: int,\n                 input_d: Tuple[int],\n                 gamma:float,\n                 batch: int = 1,\n                 activation: torch.nn.Module = torch.nn.ReLU(),\n                 head2logvar: str = 'logvar',\n                 interpolation: int = PIL.Image.BILINEAR,\n                 ) -> None:\n        super(FactorVae, self).__init__()\n        self.batch = batch\n        self.n_latent = n_latent\n        self.gamma = gamma\n        self.n_chan = n_chan\n        self.input_d = input_d\n        self.interpolation = int(interpolation)\n        self.y_2, self.x_2 = self.get_layer_size(2)\n        self.y_3, self.x_3 = self.get_layer_size(3)\n        self.y_4, self.x_4 = self.get_layer_size(4)\n        self.y_5, self.x_5 = self.get_layer_size(5)\n        self.hidden_units = self.y_5 * self.x_5 * 16\n\n        self.enc_conv1 = torch.nn.Conv2d(\n            in_channels=self.n_chan,\n            out_channels=128,\n            kernel_size=3,\n            bias=False,\n            padding='same')\n        self.enc_conv1_bn = torch.nn.BatchNorm2d(128)\n        self.enc_conv1_af = activation\n        self.enc_conv1_pool = torch.nn.MaxPool2d(\n            kernel_size=2,\n            return_indices=True,\n            ceil_mode=True)\n\n        self.enc_conv2 = torch.nn.Conv2d(\n            in_channels=128,\n            out_channels=64,\n            kernel_size=3,\n            bias=False,\n            padding='same')\n        self.enc_conv2_bn = torch.nn.BatchNorm2d(64)\n        self.enc_conv2_af = activation\n        self.enc_conv2_pool = torch.nn.MaxPool2d(\n            kernel_size=2,\n            return_indices=True,\n            ceil_mode=True)\n\n        self.enc_conv3 = torch.nn.Conv2d(\n            in_channels=64,\n            out_channels=32,\n            kernel_size=3,\n            bias=False,\n            padding='same')\n        self.enc_conv3_bn = torch.nn.BatchNorm2d(32)\n        self.enc_conv3_af = activation\n        self.enc_conv3_pool = torch.nn.MaxPool2d(\n            kernel_size=2,\n            return_indices=True,\n            ceil_mode=True)\n\n        self.enc_conv4 = torch.nn.Conv2d(\n            in_channels=32,\n            out_channels=16,\n            kernel_size=3,\n            bias=False,\n            padding='same')\n        self.enc_conv4_bn = torch.nn.BatchNorm2d(16)\n        self.enc_conv4_af = activation\n        self.enc_conv4_pool = torch.nn.MaxPool2d(\n            kernel_size=2,\n            return_indices=True,\n            ceil_mode=True)\n\n        self.enc_dense1 = torch.nn.Linear(self.hidden_units, 2048)\n        self.enc_dense1_af = activation\n        \n        self.enc_dense2 = torch.nn.Linear(2048, 1000)\n        self.enc_dense2_af = activation\n\n        self.enc_dense3 = torch.nn.Linear(1000, 250)\n        self.enc_dense3_af = activation\n\n        self.enc_dense4mu = torch.nn.Linear(250, self.n_latent)\n        self.enc_dense4mu_af = activation\n        self.enc_dense4var = torch.nn.Linear(250, self.n_latent)\n        self.enc_dense4var_af = activation\n        self.enc_head2logvar = self.Head2LogVar(head2logvar)\n\n        self.dec_dense4 = torch.nn.Linear(self.n_latent, 250)\n        self.dec_dense4_af = activation\n        \n        self.dec_dense3 = torch.nn.Linear(250, 1000)\n        self.dec_dense3_af = activation\n\n        self.dec_dense2 = torch.nn.Linear(1000, 2048)\n        self.dec_dense2_af = activation\n\n        self.dec_dense1 = torch.nn.Linear(2048, self.hidden_units)\n        self.dec_dense1_af = activation\n\n        self.dec_conv4_pool = torch.nn.MaxUnpool2d(2)\n        self.dec_conv4 = torch.nn.ConvTranspose2d(\n            in_channels=16,\n            out_channels=32,\n            kernel_size=3,\n            bias=False,\n            padding=1)\n        self.dec_conv4_bn = torch.nn.BatchNorm2d(32)\n        self.dec_conv4_af = activation\n\n        self.dec_conv3_pool = torch.nn.MaxUnpool2d(2)\n        self.dec_conv3 = torch.nn.ConvTranspose2d(\n            in_channels=32,\n            out_channels=64,\n            kernel_size=3,\n            bias=False,\n            padding=1)\n        self.dec_conv3_bn = torch.nn.BatchNorm2d(64)\n        self.dec_conv3_af = activation\n\n        self.dec_conv2_pool = torch.nn.MaxUnpool2d(2)\n        self.dec_conv2 = torch.nn.ConvTranspose2d(\n            in_channels=64,\n            out_channels=128,\n            kernel_size=3,\n            bias=False,\n            padding=1)\n        self.dec_conv2_bn = torch.nn.BatchNorm2d(128)\n        self.dec_conv2_af = activation\n\n        self.dec_conv1_pool = torch.nn.MaxUnpool2d(2)\n        self.dec_conv1 = torch.nn.ConvTranspose2d(\n            in_channels=128,\n            out_channels=self.n_chan,\n            kernel_size=3,\n            bias=False,\n            padding=1)\n        self.dec_conv1_bn = torch.nn.BatchNorm2d(self.n_chan)\n        self.dec_conv1_af = torch.nn.Sigmoid()\n        self.discriminator = torch.nn.Sequential(nn.Linear(self.n_latent, 1000),\n                                          nn.BatchNorm1d(1000),\n                                          nn.LeakyReLU(0.2),\n                                          nn.Linear(1000, 1000),\n                                          nn.BatchNorm1d(1000),\n                                          nn.LeakyReLU(0.2),\n                                          nn.Linear(1000, 1000),\n                                          nn.BatchNorm1d(1000),\n                                          nn.LeakyReLU(0.2),\n                                          nn.Linear(1000, 2))\n        self.D_z_reserve = None\n    def encode(self, x: torch.Tensor) -> Tuple[torch.Tensor]:\n        \"\"\"Encode tensor x to its latent representation.\n        \n        Args:\n            x - batch x channels x height x width tensor.\n\n        Returns:\n            (mu, var) where mu is sample mean and var is log variance in\n            latent space.\n        \"\"\"\n        z = x\n        z = self.enc_conv1(z)\n        z = self.enc_conv1_bn(z)\n        z = self.enc_conv1_af(z)\n        z, self.indices1 = self.enc_conv1_pool(z)\n\n        z = self.enc_conv2(z)\n        z = self.enc_conv2_bn(z)\n        z = self.enc_conv2_af(z)\n        z, self.indices2 = self.enc_conv2_pool(z)\n\n        z = self.enc_conv3(z)\n        z = self.enc_conv3_bn(z)\n        z = self.enc_conv3_af(z)\n        z, self.indices3 = self.enc_conv3_pool(z)\n\n        z = self.enc_conv4(z)\n        z = self.enc_conv4_bn(z)\n        z = self.enc_conv4_af(z)\n        z, self.indices4 = self.enc_conv4_pool(z)\n\n        z = z.view(z.size(0), -1)\n        z = self.enc_dense1(z)\n        z = self.enc_dense1_af(z)\n\n        z = self.enc_dense2(z)\n        z = self.enc_dense2_af(z)\n\n        z = self.enc_dense3(z)\n        z = self.enc_dense3_af(z)\n\n        mu = self.enc_dense4mu(z)\n        mu = self.enc_dense4mu_af(mu)\n\n        pvar = self.enc_dense4var(z)\n        pvar = self.enc_dense4var_af(pvar)\n        logvar = self.enc_head2logvar(pvar)\n\n        return mu, logvar\n\n    def decode(self, z: torch.Tensor) -> torch.Tensor:\n        \"\"\"Decode a latent representation to generate a reconstructed image.\n\n        Args:\n            z - 1 x n_latent input tensor.\n\n        Returns:\n            A batch x channels x height x width tensor representing the\n            reconstructed image.\n        \"\"\"\n        y = self.dec_dense4(z)\n        y = self.dec_dense4_af(y)\n\n        y = self.dec_dense3(y)\n        y = self.dec_dense3_af(y)\n\n        y = self.dec_dense2(y)\n        y = self.dec_dense2_af(y)\n\n        y = self.dec_dense1(y)\n        y = self.dec_dense1_af(y)\n\n        y = torch.reshape(y, [self.batch, 16, self.y_5, self.x_5])\n        #y = y.view(y.size(0), -1)\n\n        y = self.dec_conv4_pool(\n            y,\n            self.indices4,\n            output_size=torch.Size([self.batch, 16, self.y_4, self.x_4]))\n        y = self.dec_conv4(y)\n        y = self.dec_conv4_bn(y)\n        y = self.dec_conv4_af(y)\n        y = self.dec_conv3_pool(\n            y,\n            self.indices3,\n            output_size=torch.Size([self.batch, 32, self.y_3, self.x_3]))\n        y = self.dec_conv3(y)\n        y = self.dec_conv3_bn(y)\n        y = self.dec_conv3_af(y)\n\n        y = self.dec_conv2_pool(\n            y,\n            self.indices2,\n            output_size=torch.Size([self.batch, 64, self.y_2, self.x_2]))\n        y = self.dec_conv2(y)\n        y = self.dec_conv2_bn(y)\n        y = self.dec_conv2_af(y)\n\n        y = self.dec_conv1_pool(\n            y,\n            self.indices1,\n            output_size=torch.Size([self.batch, 128, self.input_d[0], self.input_d[1]]))\n        y = self.dec_conv1(y)\n        y = self.dec_conv1_bn(y)\n        y = self.dec_conv1_af(y)\n        return y\n\n    def get_layer_size(self, layer: int) -> Tuple[int]:\n        \"\"\"Given a network with some input size, calculate the dimensions of\n        the resulting layers.\n        \n        Args:\n            layer - layer number (for the encoder: 1 -> 2 -> 3 -> 4, for the\n                decoder: 4 -> 3 -> 2 -> 1).\n\n        Returns:\n            (y, x) where y is the layer height in pixels and x is the layer\n            width in pixels.       \n        \"\"\"\n        y_l, x_l = self.input_d\n        for i in range(layer - 1):\n            y_l = math.ceil((y_l - 2) / 2 + 1)\n            x_l = math.ceil((x_l - 2) / 2 + 1)\n        return y_l, x_l\n\n    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor]:\n        \"\"\"Make an inference with the network.\n        \n        Args:\n            x - input image (batch x channels x height x width).\n        \n        Returns:\n            (out, mu, logvar) where:\n                out - reconstructed image (batch x channels x height x width).\n                mu - mean of sample in latent space.\n                logvar - log variance of sample in latent space.\n        \"\"\"\n        mu, logvar = self.encode(x)\n        std = torch.exp(logvar / 2)\n        eps = torch.randn_like(std)\n        z = mu + std * eps\n        out = self.decode(z)\n        return z, out, mu, logvar\n\n    def train_self(self,\n                   data_path: str,\n                   epochs: int,\n                   weights_file: str) -> None:\n        \"\"\"Train the FactorVAE network.  The learning rate is hardcoded based on\n        the original FactorVAE OOD detection paper.  This training method also\n        forces the use of a manual seed to ensure repeatability.\n        \n        Args:\n            data_path - path to training dataset.  This should be a valid\n                torch dataset with different classes for each level of each\n                partition.\n            epochs - number of epochs to train the network.\n            weights_file - name of file to save trained weights.\n        \"\"\"\n        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        print (f'Using device: {device}')\n        network = self.to(device)\n        network.eval()\n\n        torch.manual_seed(0)\n        #numpy.random.seed(0)\n        torch.use_deterministic_algorithms(False)\n        torch.backends.cudnn.benchmark = False\n        torch.cuda.manual_seed(0)\n\n        transforms = torchvision.transforms.Compose([\n            torchvision.transforms.ToTensor(),\n            torchvision.transforms.Resize(self.input_d, interpolation=self.interpolation)])\n        if self.n_chan == 1:\n            transforms = torchvision.transforms.Compose([\n                torchvision.transforms.ToTensor(),\n                torchvision.transforms.Resize(self.input_d, interpolation=self.interpolation),\n                torchvision.transforms.Grayscale()])\n        dataset = torchvision.datasets.ImageFolder(\n            root=data_path,\n            transform=transforms)\n        train_loader = torch.utils.data.DataLoader(\n            dataset=dataset,\n            batch_size=self.batch,\n            shuffle=True,\n            drop_last=True\n        )\n\n        optimizer = torch.optim.Adam(network.parameters(), lr=1e-5)\n        for epoch in range(epochs):\n            epoch_loss = 0\n            for data in train_loader:\n                input, _ = data\n                input = input.to(device)\n                z, out, mu, logvar = network(input)\n                if epoch == 75:\n                    for group in optimizer.param_groups:\n                        group['lr'] = 1e-6\n                kl_loss = torch.mul(\n                    input=torch.sum(mu.pow(2) + logvar.exp() - logvar - 1),\n                    other=0.5)\n                recons_loss = torch.nn.functional.binary_cross_entropy(\n                    input=out,\n                    target=input,\n                    reduction='sum')        \n\n                self.D_z_reserve = self.discriminator(z)\n                vae_tc_loss = (self.D_z_reserve[:, 0] - self.D_z_reserve[:, 1]).mean()\n\n                loss = recons_loss + kl_loss + self.gamma * vae_tc_loss\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n                epoch_loss += loss\n            print(f'Epoch: {epoch}; Loss: {loss}')\n\n        print('Training finished, saving weights...')\n        torch.save(network, weights_file)\n\n    \n    def mig(self, data_path: str, iters: int, weights_file:str, samples: int = 100) -> float:\n        \"\"\"Find this network's mutual information gain on a given data set.\n        \n        Args:\n            data_path - path to data set of images to on which to calculate\n                mutual information gain.\n            iters - number of iterations to sample latent space.  Higher gives\n                better accuracy at the expence of more time.\n                \n        Returns:\n            Mutual information gain of this network.\n        \"\"\"\n        # 1. Inference on the Network to get Latent Dists\n        self.batch = 1\n        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        network = torch.load(weights_file)\n        network.eval()\n        n_latent = network.n_latent\n        torch.manual_seed(0)\n        #numpy.random.seed(0)\n        torch.use_deterministic_algorithms(False)\n        torch.backends.cudnn.benchmark = False\n        torch.cuda.manual_seed(0)\n        n_latent = network.n_latent\n        transforms = torchvision.transforms.Compose([\n            torchvision.transforms.ToTensor(),\n            torchvision.transforms.Resize((224, 224))])\n        dataset = torchvision.datasets.ImageFolder(\n            root=data_path,\n            transform=transforms)\n        cv_loader = torch.utils.data.DataLoader(\n            dataset=dataset,\n            batch_size=self.batch,\n            shuffle=True,\n            drop_last=True)\n        mu = numpy.zeros((self.n_latent, len(dataset.imgs)))\n        logvar = numpy.zeros((self.n_latent, len(dataset.imgs)))\n        f_mask = numpy.zeros(len(dataset.imgs))\n        f_counts = {f: 0 for _, f in dataset.class_to_idx.items()}\n        idx = 0\n        for data in cv_loader:\n            x, partition = data\n            x = x.to(device)\n            _, _, m, lv = network(x)\n            mu[:, idx] = m.detach().cpu().numpy()\n            logvar[:, idx] = lv.detach().cpu().numpy()\n            f_mask[idx] = partition\n            f_counts[int(partition)] += 1\n            idx += 1\n\n        f_probs = [count / len(dataset.imgs) for _, count in f_counts.items()]\n        f_entropy = scipy.stats.entropy(f_probs)\n        migs = numpy.zeros(iters)\n        final_res = [0 for i in range(n_latent)]\n        for i in range(iters):\n            iter_res = [0 for i in range(n_latent)]\n            print(f'Getting MIG for Iter {i}')\n            std = numpy.exp(logvar / 2)\n            smp = numpy.zeros((self.n_latent, len(dataset.imgs), samples))\n            # Get samples for all images latent dists\n            for s in range(samples):\n                eps = numpy.random.randn(*std.shape)\n                smp[:, :, s] = mu + std * eps\n\n            # Get probability of each sample occurring\n            p_lat = numpy.zeros((self.n_latent, len(dataset.imgs), samples))\n            for lat in range(self.n_latent):\n                for d in range(len(dataset.imgs)):\n                    sig = numpy.exp(logvar[lat, d] / 2)\n                    p_lat[lat, d, :] = (1 / (sig * numpy.sqrt(2 * numpy.pi))) \\\n                            * numpy.exp(-0.5 * ((smp[lat, d, :] - mu[lat, d]) \\\n                            / sig) ** 2)\n\n            h_lat = numpy.zeros(self.n_latent)\n            # Get the entropies of each latent variable across whole input set\n            for lat in range(self.n_latent):\n                h_lat[lat] = scipy.stats.entropy(p_lat[lat, :, :].flatten())\n                mig = 0\n            for f in f_counts:\n                h_lat_given_f = numpy.zeros(self.n_latent)\n                class_length = 0\n                for lat in range(self.n_latent):\n                    p_lat_given_f = numpy.zeros((f_counts[f], samples))\n                    idx = 0\n                    for d in range(len(dataset.imgs)):\n                        if f_mask[d] == f:\n                            p_lat_given_f[idx, :] = p_lat[lat, d, :]\n                            idx += 1\n                    h_lat_given_f[lat] = scipy.stats.entropy(p_lat_given_f.flatten())\n                mi = h_lat - h_lat_given_f\n                for idx in range(n_latent):\n                    iter_res[idx] += mi[idx]\n#                 mi.sort()\n                \n#                 mig += (mi[-1] - mi[-2]) / f_entropy\n#             print(iter_res)\n            iter_res = numpy.divide(iter_res,11)\n            for idx in range(n_latent):\n                final_res[idx] += iter_res[idx]\n#             migs[i] = 1 / len(dataset.classes) * mig\n#         print(migs)\n        final_res = numpy.divide(final_res,iters)\n#         print(final_res)\n        os.remove(weights_file)\n        return numpy.mean(final_res)\n\n    class Head2LogVar:\n        \"\"\"This class defines the final layer on one of the encoder heads.\n        Essentially it performs an element-wise operation on the output of\n        each neuron in the preceding layer in order to transform the input\n        to log(var).\n\n        Args:\n            logvar - transform from what to logvar: 'logvar', 'logvar+1',\n                'neglogvar', or 'var'.\n        \"\"\"\n\n        def __init__(self, type: str = 'logvar'):\n            self.eps = 1e-6\n            self.type = {\n                'logvar': self.logvar,\n                'logvar+1': self.logvarplusone,\n                'neglogvar': self.neglogvar,\n                'var': self.var}[type]\n\n        def logvar(self, x: torch.Tensor):\n            \"\"\"IF x == log(sig^2):\n            THEN x = log(sig^2)\"\"\"\n            return x\n\n        def logvarplusone(self, x: torch.Tensor):\n            \"\"\"IF x = log(sig^2 + 1)\n            THEN log(e^x - 1) = log(sig^2)\"\"\"\n            return x.exp().add(-1 + self.eps).log()\n\n        def neglogvar(self, x: torch.Tensor):\n            \"\"\"IF x = -log(sig^2)\n            THEN -x = log(sig^2)\"\"\"\n            return x.neg()\n\n        def var(self, x: torch.Tensor):\n            \"\"\"IF x = sig^2\n            THEN log(x) = log(sig^2)\"\"\"\n            return x.add(self.eps).log()\n\n        def __call__(self, input: torch.Tensor):\n            \"\"\"Runs when calling instance of object.\"\"\"\n            return self.type(input)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def mig(n, gamma) -> float:\n    \"\"\"Find the mutual information gain for an instance of the beta\n    variational autoencoder network.\n\n    Args:\n        n: number of latent dimensions\n        beta: weight of KL-divergence loss during training\n    \"\"\"\n    n = int(n)\n    weights_file=f'bvae_n{n}_b{gamma}_{\"bw\" if N_CHAN == 1 else \"\"}_{INPUT_DIM[0]}x{INPUT_DIM[1]}.pt'\n    model = FactorVae(\n        n_latent=n,\n        n_chan=N_CHAN,\n        input_d=INPUT_DIM,\n        batch=BATCH,\n        gamma = gamma,\n        )\n    model.train_self(\n        data_path=TRAIN_PATH,\n        epochs=1,\n        weights_file=weights_file)\n    return model.mig(CV_PATH, 1,weights_file)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def optimize_mig():\n    \"\"\"Find the parameter n_latent  and beta that maximize the mutual\n    information gain of a beta variational autoencoder\"\"\"\n    optimizer = BayesianOptimization(\n        f=mig,\n        pbounds={'n': (5, 200), 'gamma': (0.001, 30)},\n        verbose=2,\n        random_state=1)\n    optimizer.maximize(n_iter=1)\n    print('#################################################################')\n    print(f'Found Network with Optimal MIG of {optimizer.max[\"target\"]}')\n    print(f'Parameters: {optimizer.max[\"params\"]}')\n    print('#################################################################')\n    return optimizer.max[\"params\"]\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TRAIN_PATH = '' #Link to Train dataset\nN_CHAN = 3\nINPUT_DIM = (224,224)\nBATCH = 1\nCV_PATH = '' #Link to CV dataset\nbest_hypers = optimize_mig() ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"best_hypers","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(n,gamma) -> float:\n    \"\"\"Find the mutual information gain for an instance of the beta\n    variational autoencoder network.\n\n    Args:\n        n: number of latent dimensions\n        beta: weight of KL-divergence loss during training\n    \"\"\"\n    n = int(n)\n    model = FactorVae(\n        n_latent=n,\n        n_chan=N_CHAN,\n        input_d=INPUT_DIM,\n        batch=BATCH,\n        gamma = gamma)\n    model.train_self(\n        data_path=TRAIN_PATH,\n        epochs=EPOCHS,\n        weights_file=f'factorVAE.pt')\n    return \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"Find the number of latent dimensions and the beta value that maximize the\nmutual information gain for a Beta VAE.\"\"\"\n\n\nfrom scipy.stats import gaussian_kde\n\n\ndef test(n, gamma) -> float:\n    \"\"\"Find the mutual information gain for an instance of the beta\n    variational autoencoder network.\n\n    Args:\n        n: number of latent dimensions\n        beta: weight of KL-divergence loss during training\n    \"\"\"\n    n = int(n)\n    model = FactorVae(\n        n_latent=n,\n        n_chan=N_CHAN,\n        input_d=INPUT_DIM,\n        batch=BATCH,\n        gamma = gamma)\n    res = model.test(\n        data_path=TEST_PATH,\n        epochs=1,\n        model_file=f'factorVAE.pt')\n    return res","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, roc_curve\ndef train_model(res_dict):\n    n = res_dict['n']\n    gamma = res_dict['gamma']\n    train(n, gamma)\n\ndef get_roc_score(y_prob, y_true):\n    y_prob_arr = []\n    for item in y_prob:\n        item = item.detach().cpu().numpy()\n        y_prob_arr.append(item)\n    y_true_arr = []\n    for item in y_true:\n        item = item.detach().cpu().numpy()\n        y_true_arr.append(item)\n    score = roc_auc_score(y_true_arr,y_prob_arr)\n    return score\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_hypers = {'gamma': 5.58862008111875, 'n': 72.38434177339431}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TRAIN_PATH = '/kaggle/input/carla-mig/FYP_Train/FYP_Train'\nN_CHAN = 3\nINPUT_DIM = (224,224)\nBATCH = 1\nEPOCHS = 1\ntrain_model(best_hypers)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_scene_dkls(scene: str, network: torch.nn.Module) -> List[List[float]]:\n    \"\"\"Get the KL divergence for each frame in a given scene.\n    \n    Args:\n        scene - path to scene (video file) to process.\n        network - torch model whose output is a tuple where (out, mu, logvar)\n            represents: *out* - decoder output, *mu* - sample mean in latent\n            space, and *logvar* - sample log variance in latent space.\n    \n    Returns:\n        A list of (1 x n_latent) lists corresponding to the KL divergence for\n        each frame in the scene for each latent variable in the model.\n    \"\"\"\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    dkls = []\n    vid_in = cv2.VideoCapture(scene)\n    while vid_in.isOpened():\n        ret, frame = vid_in.read()\n        if not ret:\n            break\n        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n        frame = torchvision.transforms.functional.to_tensor(frame)\n        frame = torchvision.transforms.functional.resize(\n                frame,\n                network.input_d,\n                int(network.interpolation))\n        if network.n_chan == 1:\n            frame = torchvision.transforms.functional.rgb_to_grayscale(frame)\n        _,_, mu, logvar = network(frame.unsqueeze(0).to(device))\n        mu = mu.detach().cpu().numpy()\n        logvar = logvar.detach().cpu().numpy()\n        dkl = 0.5 * (numpy.power(mu, 2) + numpy.exp(logvar) - logvar - 1)\n        dkls.append(numpy.squeeze(dkl).tolist())\n    return dkls\n\n\ndef get_scene_kl_diff(dkls: List[List[float]]) -> numpy.ndarray:\n    \"\"\"Get the mean KLdiff for a given scene.\n    \n    Args:\n        dkls - A list of KL divergeneces for each frame in a scene.\n\n    Returns:\n        A (1 x n_latent) array of mean KLdiff for each latent dimension for\n        the provided scene and model.\n    \"\"\"\n    kl_curr = None\n    kl_next = None\n    scene_length = 0\n    scene_mean = numpy.zeros((1, len(dkls[0])))\n    for frame in dkls:\n        kl_curr = kl_next\n        kl_next = numpy.array(frame).reshape((1, len(frame)))\n        if kl_curr is None:\n            continue\n        else:\n            kl_diff = numpy.abs(kl_next - kl_curr)\n            scene_length +=1\n            scene_mean += (kl_diff - scene_mean) / scene_length\n    return scene_mean\n\n\ndef get_partition_variance(partition_path: str,\n                           network: torch.nn.Module) -> Dict[str,\n                                                             List[Tuple]]:\n    \"\"\"Get the variance of KLdiff for a partition.\n    \n    Args:\n        partition_path - path to partition in calibration set.\n        network - torch model whose output is a tuple where (out, mu, logvar)\n            represents: *out* - decoder output, *mu* - sample mean in latent\n            space, and *logvar* - sample log variance in latent space.\n    \n    Returns:\n        Dictionary with keys:\n            dkls - list of KL divergences for each latent dimension in each\n                frame in the partition.\n            top_z - sorted list of tuples of the form (latent dimension, \n                variance), where the first tuple is the latent dimension with\n                the highest variance.\n    \"\"\"\n    partition_stats = {'dkls': [], 'top_z': []}\n    wellford_m2 = numpy.zeros((1, network.n_latent))\n    wellford_mean = numpy.zeros((1, network.n_latent))\n    wellford_count = 0\n    for scene in os.listdir(partition_path):\n        dkls = get_scene_dkls(os.path.join(partition_path, scene), network)\n        partition_stats['dkls'].extend(dkls)\n        scene_mean = get_scene_kl_diff(dkls)\n        wellford_count += 1\n        delta = scene_mean - wellford_mean\n        wellford_mean = wellford_mean + delta / wellford_count\n        delta2 = scene_mean - wellford_mean\n        wellford_m2 = delta * delta2\n    variance = wellford_m2 / wellford_count\n    variance = numpy.squeeze(variance).tolist()\n    for _ in range(len(variance)):\n        idx, val = max(enumerate(variance), key=lambda x: x[1])\n        partition_stats['top_z'].append((idx, val))\n        variance[idx] = -1\n    return partition_stats\n\n\ndef calibrate(weights: str, dataset: str) -> None:\n    \"\"\"Calibrate all partitions.\n    \n    Args:\n        weights - path to weights file for the network.\n        dataset - path to calibration dataset.\n    \"\"\"\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f'Using device: {device}')\n    network = torch.load(weights)\n    network.batch = 1\n    network.eval()\n    network.to(device)\n    alpha_cal = {}\n    alpha_cal['PARAMS'] = {}\n    alpha_cal['PARAMS']['n_latent'] = int(network.n_latent)\n    alpha_cal['PARAMS']['input_d'] = tuple(int(x) for x in network.input_d)\n    alpha_cal['PARAMS']['n_chan'] = int(network.n_chan)\n    alpha_cal['PARAMS']['interpolation'] = int(network.interpolation)\n    for partition in os.listdir(dataset):\n        print(f'Processing partition: {partition}')\n        alpha_cal[partition] = get_partition_variance(\n            os.path.join(dataset, partition),\n            network)\n        print(f'Rankings for partition: {partition}')\n        for rank, value in enumerate(alpha_cal[partition]['top_z']):\n            print(f'{rank}: {value}')\n    dest_path = list(os.path.split(weights))\n    dest_path[-1] = f'alpha_cal_{dest_path[-1].replace(\"pt\", \"json\")}'        \n    with open(os.path.join(*dest_path), 'w') as alpha_cal_f:\n        alpha_cal_f.write(json.dumps(alpha_cal))\n\n\nweights_file = ['factorVAE.pt']\ndataset = '/kaggle/input/brightness-data/brightness_video'\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f'Using device: {device}')\n\n\nfor weights in weights_file:  \n    network = torch.load(weights)\n    print(f'Processing file: {weights}')\n    network.batch = 1\n    network.eval()\n    network.to(device)\n    alpha_cal = {}\n    alpha_cal['PARAMS'] = {}\n    alpha_cal['PARAMS']['n_latent'] = network.n_latent\n    alpha_cal['PARAMS']['input_d'] = network.input_d\n    alpha_cal['PARAMS']['n_chan'] = network.n_chan\n    for partition in os.listdir(dataset):\n        print(f'Processing partition: {partition}')\n        alpha_cal[partition] = get_partition_variance(\n            os.path.join(dataset, partition),\n                network)\n        print(f'Rankings for partition: {partition}')\n        for rank, value in enumerate(alpha_cal[partition]['top_z']):\n                print(f'{rank}: {value}')\n\ndataset = '/kaggle/input/carla-mig/calibration/calibration'\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f'Using device: {device}')\n\n\nfor weights in weights_file:  \n    network = torch.load(weights)\n    print(f'Processing file: {weights}')\n    network.batch = 1\n    network.eval()\n    network.to(device)\n    alpha_cal = {}\n    alpha_cal['PARAMS'] = {}\n    alpha_cal['PARAMS']['n_latent'] = network.n_latent\n    alpha_cal['PARAMS']['input_d'] = network.input_d\n    alpha_cal['PARAMS']['n_chan'] = network.n_chan\n    for partition in os.listdir(dataset):\n        print(f'Processing partition: {partition}')\n        alpha_cal[partition] = get_partition_variance(\n            os.path.join(dataset, partition),\n                network)\n        print(f'Rankings for partition: {partition}')\n        for rank, value in enumerate(alpha_cal[partition]['top_z']):\n                print(f'{rank}: {value}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"latent_to_keep = []\nfor rank, value in enumerate(alpha_cal[partition]['top_z']):\n    if value[1] > 0:\n        latent_to_keep.append(value[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from scipy.stats import gaussian_kde\nfrom bayes_opt import BayesianOptimization\n\n#!/usr/bin/env python3\n\n\n\nfrom typing import Callable, Tuple, List\nimport argparse\nimport math\nimport sys\nimport numpy\nimport scipy.stats\nimport torch\nimport torchvision\nimport PIL\nimport matplotlib.pyplot as plt\nimport torch.nn.functional as F\nclass FactorVae(torch.nn.Module):\n    \"\"\"FactorVae OOD detector.  This class includes both the encoder and\n    decoder portions of the model.\n    \n    Args:\n        n_latent - number of latent dimensions\n        beta - hyperparameter beta to use during training\n        n_chan - number of channels in the input image\n        input_d - height x width tuple of input image size in pixels\n        activation - activation function to use for all hidden layers\n        head2logvar - 2nd distribution parameter learned by encoder:\n            'logvar', 'logvar+1', 'var', or 'neglogvar'.\n        interpolation - PIL image interpolation method on resize\n    \"\"\"\n    num_iter = 0 # Global static variable to keep track of iterations\n\n    def __init__(self,\n                 n_latent: int,\n                 n_chan: int,\n                 input_d: Tuple[int],\n                 latent_list: List[int],\n                 batch: int = 1,\n                 activation: torch.nn.Module = torch.nn.ReLU(),\n                 head2logvar: str = 'logvar',\n                 interpolation: int = PIL.Image.BILINEAR,\n                 Capacity_max_iter: int = 1e5,\n                 alpha: float = 1.,\n                 beta: float =  6.,\n                 gamma: float = 1.,\n                 max_capacity: int = 25) -> None:\n        super(FactorVae, self).__init__()\n        self.alpha = alpha\n        self.beta = beta\n        self.C_max = torch.Tensor([max_capacity])\n        self.C_stop_iter = Capacity_max_iter\n        self.gamma = gamma\n        self.batch = batch\n        self.n_latent = n_latent\n        self.beta = beta\n        self.n_chan = n_chan\n        self.input_d = input_d\n        self.interpolation = int(interpolation)\n        self.y_2, self.x_2 = self.get_layer_size(2)\n        self.y_3, self.x_3 = self.get_layer_size(3)\n        self.y_4, self.x_4 = self.get_layer_size(4)\n        self.y_5, self.x_5 = self.get_layer_size(5)\n        self.hidden_units = self.y_5 * self.x_5 * 16\n\n        self.enc_conv1 = torch.nn.Conv2d(\n            in_channels=self.n_chan,\n            out_channels=128,\n            kernel_size=3,\n            bias=False,\n            padding='same')\n        self.enc_conv1_bn = torch.nn.BatchNorm2d(128)\n        self.enc_conv1_af = activation\n        self.enc_conv1_pool = torch.nn.MaxPool2d(\n            kernel_size=2,\n            return_indices=True,\n            ceil_mode=True)\n\n        self.enc_conv2 = torch.nn.Conv2d(\n            in_channels=128,\n            out_channels=64,\n            kernel_size=3,\n            bias=False,\n            padding='same')\n        self.enc_conv2_bn = torch.nn.BatchNorm2d(64)\n        self.enc_conv2_af = activation\n        self.enc_conv2_pool = torch.nn.MaxPool2d(\n            kernel_size=2,\n            return_indices=True,\n            ceil_mode=True)\n\n        self.enc_conv3 = torch.nn.Conv2d(\n            in_channels=64,\n            out_channels=32,\n            kernel_size=3,\n            bias=False,\n            padding='same')\n        self.enc_conv3_bn = torch.nn.BatchNorm2d(32)\n        self.enc_conv3_af = activation\n        self.enc_conv3_pool = torch.nn.MaxPool2d(\n            kernel_size=2,\n            return_indices=True,\n            ceil_mode=True)\n\n        self.enc_conv4 = torch.nn.Conv2d(\n            in_channels=32,\n            out_channels=16,\n            kernel_size=3,\n            bias=False,\n            padding='same')\n        self.enc_conv4_bn = torch.nn.BatchNorm2d(16)\n        self.enc_conv4_af = activation\n        self.enc_conv4_pool = torch.nn.MaxPool2d(\n            kernel_size=2,\n            return_indices=True,\n            ceil_mode=True)\n\n        self.enc_dense1 = torch.nn.Linear(self.hidden_units, 2048)\n        self.enc_dense1_af = activation\n        \n        self.enc_dense2 = torch.nn.Linear(2048, 1000)\n        self.enc_dense2_af = activation\n\n        self.enc_dense3 = torch.nn.Linear(1000, 250)\n        self.enc_dense3_af = activation\n\n        self.enc_dense4mu = torch.nn.Linear(250, self.n_latent)\n        self.enc_dense4mu_af = activation\n        self.enc_dense4var = torch.nn.Linear(250, self.n_latent)\n        self.enc_dense4var_af = activation\n        self.enc_head2logvar = self.Head2LogVar(head2logvar)\n\n        self.dec_dense4 = torch.nn.Linear(self.n_latent, 250)\n        self.dec_dense4_af = activation\n        \n        self.dec_dense3 = torch.nn.Linear(250, 1000)\n        self.dec_dense3_af = activation\n\n        self.dec_dense2 = torch.nn.Linear(1000, 2048)\n        self.dec_dense2_af = activation\n\n        self.dec_dense1 = torch.nn.Linear(2048, self.hidden_units)\n        self.dec_dense1_af = activation\n\n        self.dec_conv4_pool = torch.nn.MaxUnpool2d(2)\n        self.dec_conv4 = torch.nn.ConvTranspose2d(\n            in_channels=16,\n            out_channels=32,\n            kernel_size=3,\n            bias=False,\n            padding=1)\n        self.dec_conv4_bn = torch.nn.BatchNorm2d(32)\n        self.dec_conv4_af = activation\n\n        self.dec_conv3_pool = torch.nn.MaxUnpool2d(2)\n        self.dec_conv3 = torch.nn.ConvTranspose2d(\n            in_channels=32,\n            out_channels=64,\n            kernel_size=3,\n            bias=False,\n            padding=1)\n        self.dec_conv3_bn = torch.nn.BatchNorm2d(64)\n        self.dec_conv3_af = activation\n\n        self.dec_conv2_pool = torch.nn.MaxUnpool2d(2)\n        self.dec_conv2 = torch.nn.ConvTranspose2d(\n            in_channels=64,\n            out_channels=128,\n            kernel_size=3,\n            bias=False,\n            padding=1)\n        self.dec_conv2_bn = torch.nn.BatchNorm2d(128)\n        self.dec_conv2_af = activation\n\n        self.dec_conv1_pool = torch.nn.MaxUnpool2d(2)\n        self.dec_conv1 = torch.nn.ConvTranspose2d(\n            in_channels=128,\n            out_channels=self.n_chan,\n            kernel_size=3,\n            bias=False,\n            padding=1)\n        self.dec_conv1_bn = torch.nn.BatchNorm2d(self.n_chan)\n        self.dec_conv1_af = torch.nn.Sigmoid()\n\n    def encode(self, x: torch.Tensor) -> Tuple[torch.Tensor]:\n        \"\"\"Encode tensor x to its latent representation.\n        \n        Args:\n            x - batch x channels x height x width tensor.\n\n        Returns:\n            (mu, var) where mu is sample mean and var is log variance in\n            latent space.\n        \"\"\"\n        z = x\n        z = self.enc_conv1(z)\n        z = self.enc_conv1_bn(z)\n        z = self.enc_conv1_af(z)\n        z, self.indices1 = self.enc_conv1_pool(z)\n#         print(\"Z1: \",z )\n        z = self.enc_conv2(z)\n        z = self.enc_conv2_bn(z)\n        z = self.enc_conv2_af(z)\n        z, self.indices2 = self.enc_conv2_pool(z)\n#         print(\"Z2: \",z )\n        z = self.enc_conv3(z)\n        z = self.enc_conv3_bn(z)\n        z = self.enc_conv3_af(z)\n        z, self.indices3 = self.enc_conv3_pool(z)\n#         print(\"Z3: \",z )\n        z = self.enc_conv4(z)\n        z = self.enc_conv4_bn(z)\n        z = self.enc_conv4_af(z)\n        z, self.indices4 = self.enc_conv4_pool(z)\n#         print(\"Z4: \",z )\n        z = z.view(z.size(0), -1)\n        z = self.enc_dense1(z)\n        z = self.enc_dense1_af(z)\n#         print(\"Z5: \",z )\n        z = self.enc_dense2(z)\n        z = self.enc_dense2_af(z)\n#         print(\"Z6: \",z )\n        z = self.enc_dense3(z)\n        z = self.enc_dense3_af(z)\n#         print(\"Z7 \",z )\n        mu = self.enc_dense4mu(z)\n        mu = self.enc_dense4mu_af(mu)\n\n        pvar = self.enc_dense4var(z)\n        pvar = self.enc_dense4var_af(pvar)\n        logvar = self.enc_head2logvar(pvar)\n        return mu, logvar\n\n    def decode(self, z: torch.Tensor) -> torch.Tensor:\n        \"\"\"Decode a latent representation to generate a reconstructed image.\n\n        Args:\n            z - 1 x n_latent input tensor.\n\n        Returns:\n            A batch x channels x height x width tensor representing the\n            reconstructed image.\n        \"\"\"\n        y = self.dec_dense4(z)\n        y = self.dec_dense4_af(y)\n\n        y = self.dec_dense3(y)\n        y = self.dec_dense3_af(y)\n\n        y = self.dec_dense2(y)\n        y = self.dec_dense2_af(y)\n\n        y = self.dec_dense1(y)\n        y = self.dec_dense1_af(y)\n\n        y = torch.reshape(y, [self.batch, 16, self.y_5, self.x_5])\n        #y = y.view(y.size(0), -1)\n\n        y = self.dec_conv4_pool(\n            y,\n            self.indices4,\n            output_size=torch.Size([self.batch, 16, self.y_4, self.x_4]))\n        y = self.dec_conv4(y)\n        y = self.dec_conv4_bn(y)\n        y = self.dec_conv4_af(y)\n\n        y = self.dec_conv3_pool(\n            y,\n            self.indices3,\n            output_size=torch.Size([self.batch, 32, self.y_3, self.x_3]))\n        y = self.dec_conv3(y)\n        y = self.dec_conv3_bn(y)\n        y = self.dec_conv3_af(y)\n\n        y = self.dec_conv2_pool(\n            y,\n            self.indices2,\n            output_size=torch.Size([self.batch, 64, self.y_2, self.x_2]))\n        y = self.dec_conv2(y)\n        y = self.dec_conv2_bn(y)\n        y = self.dec_conv2_af(y)\n\n        y = self.dec_conv1_pool(\n            y,\n            self.indices1,\n            output_size=torch.Size([self.batch, 128, self.input_d[0], self.input_d[1]]))\n        y = self.dec_conv1(y)\n        y = self.dec_conv1_bn(y)\n        y = self.dec_conv1_af(y)\n        return y\n\n    def get_layer_size(self, layer: int) -> Tuple[int]:\n        \"\"\"Given a network with some input size, calculate the dimensions of\n        the resulting layers.\n        \n        Args:\n            layer - layer number (for the encoder: 1 -> 2 -> 3 -> 4, for the\n                decoder: 4 -> 3 -> 2 -> 1).\n\n        Returns:\n            (y, x) where y is the layer height in pixels and x is the layer\n            width in pixels.       \n        \"\"\"\n        y_l, x_l = self.input_d\n        for i in range(layer - 1):\n            y_l = math.ceil((y_l - 2) / 2 + 1)\n            x_l = math.ceil((x_l - 2) / 2 + 1)\n        return y_l, x_l\n\n    def forward(self, x: torch.Tensor, n_latent) -> Tuple[torch.Tensor]:\n        \"\"\"Make an inference with the network.\n        \n        Args:\n            x - input image (batch x channels x height x width).\n        \n        Returns:\n            (out, mu, logvar) where:\n                out - reconstructed image (batch x channels x height x width).\n                mu - mean of sample in latent space.\n                logvar - log variance of sample in latent space.\n        \"\"\"\n        mu, logvar = self.encode(x)\n#         print('x: ',x)\n#         print('mu: ', mu)\n#         print('logvar: ', logvar)\n        std = torch.exp(logvar / 2)\n        eps = torch.randn_like(std)\n        z = mu + std * eps\n        for i in range(n_latent):\n            if i not in latent_to_keep:\n                mu[0][i] = 0\n                logvar[0][i] = 0\n#         latent_to_remove = [16]\n#         for i in range(n_latent):\n#             if i in latent_to_remove:\n#                 mu[0][i] = 0\n#                 logvar[0][i] = 0                \n        \n        out = self.decode(z)\n        return z, out, mu, logvar\n    \n    def log_density_gaussian(self, x: torch.Tensor, mu: torch.Tensor, logvar: torch.Tensor):\n        \"\"\"\n        Computes the log pdf of the Gaussian with parameters mu and logvar at x\n        :param x: (Tensor) Point at whichGaussian PDF is to be evaluated\n        :param mu: (Tensor) Mean of the Gaussian distribution\n        :param logvar: (Tensor) Log variance of the Gaussian distribution\n        :return:\n        \"\"\"\n        norm = - 0.5 * (math.log(2 * math.pi) + logvar)\n        log_density = norm - 0.5 * ((x - mu) ** 2 * torch.exp(-logvar))\n        return log_density\n\n    def test(self,\n                   data_path: str,\n                   model_file: str) -> None:\n        \"\"\"Train the FactorVae network.  The learning rate is hardcoded based on\n        the original FactorVae OOD detection paper.  This training method also\n        forces the use of a manual seed to ensure repeatability.\n\n        Args:\n            data_path - path to training dataset.  This should be a valid\n                torch dataset with different classes for each level of each\n                partition.\n            epochs - number of epochs to train the network.\n            weights_file - name of file to save trained weights.\n        \"\"\"\n        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        print(f'Using device: {device}')\n        #network = self.to(device)\n        network = torch.load(model_file)\n        network.eval()\n        n_latent = network.n_latent\n        torch.manual_seed(0)\n        #numpy.random.seed(0)\n        torch.use_deterministic_algorithms(False)\n        torch.backends.cudnn.benchmark = False\n        torch.cuda.manual_seed(0)\n\n        transforms = torchvision.transforms.Compose([\n            torchvision.transforms.ToTensor(),\n            torchvision.transforms.Resize(self.input_d, interpolation=self.interpolation)])\n        \n        dataset = torchvision.datasets.ImageFolder(\n            root=data_path,\n            transform=transforms)\n        print(dataset.class_to_idx)\n        print(\"Batch \", self.batch)\n        test_loader = torch.utils.data.DataLoader(\n            dataset=dataset,\n            batch_size=self.batch,\n            shuffle=True,\n            drop_last=True)\n        mu_ID = []\n        logvar_ID = []\n        z_ID_loss = []\n        z_OOD_loss = []\n        y_prob = []\n        y_true = []\n        mu_OOD = []\n        logvar_OOD = []\n        for data in test_loader:\n            input, y = data #input = image, y=class\n            input = input.to(device)\n            y_true.append(y)\n            with torch.no_grad():\n                z, out, mu, logvar = network(input, n_latent)\n#                 print(out)\n#                 print(torch.sum(mu.pow(2) + logvar.exp() - logvar - 1))\n                kl_loss = torch.mul(\n                input=torch.sum(mu.pow(2) + logvar.exp() - logvar - 1),\n                            other=0.5)\n#                 loss = torch.nn.KLDivLoss()\n#                 kl_loss = loss(input, out)\n#                 kl_loss = torch.nn.functional.binary_cross_entropy(\n#                     input=out,\n#                     target=input,\n#                     reduction='sum')\n                if (y == 0):\n                    mu_ID.append(mu)\n                    logvar_ID.append(logvar)\n                    z_ID_loss.append(kl_loss)\n                else:\n                    mu_OOD.append(mu)\n                    logvar_OOD.append(logvar)\n                    z_OOD_loss.append(kl_loss)\n                y_prob.append(kl_loss)\n                        #print('kl_loss:', kl_loss)\n                #print(f'Epoch: {epoch}; Total KL_loss: {kl_loss}')\n        print(\"Total loss found:\", len(y_prob))\n        print('Testing finished, saving results...')\n        return y_prob, y_true, mu_ID, logvar_ID, mu_OOD, logvar_OOD, z_ID_loss, z_OOD_loss\n\n    class Head2LogVar:\n        \"\"\"This class defines the final layer on one of the encoder heads.\n        Essentially it performs an element-wise operation on the output of\n        each neuron in the preceding layer in order to transform the input\n        to log(var).\n\n        Args:\n            logvar - transform from what to logvar: 'logvar', 'logvar+1',\n                'neglogvar', or 'var'.\n        \"\"\"\n\n        def __init__(self, type: str = 'logvar'):\n            self.eps = 1e-6\n            self.type = {\n                'logvar': self.logvar,\n                'logvar+1': self.logvarplusone,\n                'neglogvar': self.neglogvar,\n                'var': self.var}[type]\n\n        def logvar(self, x: torch.Tensor):\n            \"\"\"IF x == log(sig^2):\n            THEN x = log(sig^2)\"\"\"\n            return x\n\n        def logvarplusone(self, x: torch.Tensor):\n            \"\"\"IF x = log(sig^2 + 1)\n            THEN log(e^x - 1) = log(sig^2)\"\"\"\n            return x.exp().add(-1 + self.eps).log()\n        def neglogvar(self, x: torch.Tensor):\n            \"\"\"IF x = -log(sig^2)\n            THEN -x = log(sig^2)\"\"\"\n            return x.neg()\n\n        def var(self, x: torch.Tensor):\n            \"\"\"IF x = sig^2\n            THEN log(x) = log(sig^2)\"\"\"\n            return x.add(self.eps).log()\n\n        def __call__(self, input: torch.Tensor):\n            \"\"\"Runs when calling instance of object.\"\"\"\n            return self.type(input)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"weights_file = 'factorVAE.pt'\nnetwork = torch.load(weights_file)\nTEST_PATH = '/kaggle/input/carla-test/Test/Test' #Link to test dataset\ny_prob, y_true, mu_ID, logvar_ID, mu_OOD, logvar_OOD, z_ID_loss, z_OOD_loss = network.test(TEST_PATH, weights_file)\nget_roc_score(y_prob, y_true)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in latent_to_keep:\n    print(\"Latent \",i)\n    x = []\n    y = []\n    x1 = []\n    y1 = []\n    for item in mu_ID:\n        x.append(item.detach().cpu().numpy()[0][i])\n    for item in logvar_ID:\n        y.append(item.detach().cpu().numpy()[0][i])\n    for item in mu_OOD:\n        x1.append(item.detach().cpu().numpy()[0][i])\n    for item in logvar_OOD:\n        y1.append(item.detach().cpu().numpy()[0][i])\n    fig = plt.figure()\n    ax1 = fig.add_subplot(111)\n\n    ax1.scatter(x, y, s=10, c='b', marker=\"s\", label='ID')\n    ax1.scatter(x1, y1, s=10, c='r', marker=\"o\", label='OOD')\n    plt.legend(loc='upper left')\n    plt.xlabel('MU')\n    plt.ylabel('logvar')\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}